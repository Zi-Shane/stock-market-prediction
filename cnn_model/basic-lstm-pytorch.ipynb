{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load date\n",
    "\n",
    "- `class ClosePriceDataset()`: `Dataset`\n",
    "    - `__init__`: read_csv()\n",
    "        - `MinMaxScaler`: [-1, 1]，(可以被reverse回原值)\n",
    "- `def train_valid_split`: return `Sampler`, split training data(80%) and validation data(20%)\n",
    "- `train_loader`: return `DataLoader`\n",
    "- `val_loader`: return `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1958 entries, 2010-01-04 to 2017-10-11\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Open     1958 non-null   float64\n",
      " 1   High     1958 non-null   float64\n",
      " 2   Low      1958 non-null   float64\n",
      " 3   Close    1958 non-null   float64\n",
      " 4   Volume   1958 non-null   int64  \n",
      " 5   OpenInt  1958 non-null   int64  \n",
      "dtypes: float64(4), int64(2)\n",
      "memory usage: 107.1 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2028 entries, 2010-01-04 to 2017-10-11\n",
      "Freq: B\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Open     1958 non-null   float64\n",
      " 1   High     1958 non-null   float64\n",
      " 2   Low      1958 non-null   float64\n",
      " 3   Close    1958 non-null   float64\n",
      " 4   Volume   1958 non-null   float64\n",
      " 5   OpenInt  1958 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 175.4 KB\n"
     ]
    }
   ],
   "source": [
    "from operator import index\n",
    "\n",
    "\n",
    "dates = pd.date_range('2010-01-02','2017-10-11', freq='B')\n",
    "df_dates = pd.DataFrame(index=dates)\n",
    "df_ibm = pd.read_csv(\"../datasets/ibm.csv\", parse_dates=True, index_col='Date')\n",
    "df1 = df_ibm.join(df_dates, how='inner')\n",
    "df2 = df_dates.join(df_ibm)\n",
    "df1.info()\n",
    "df2.info()\n",
    "# df_ibm.head()\n",
    "# df_ibm[['Close']].plot(figsize=(10, 6))\n",
    "# plt.title = 'IBM Stock Price'\n",
    "# plt.xlabel = 'Date'\n",
    "# plt.ylabel = 'Price'\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "每一層需給定的input（計算上一層output）的size\n",
    "\n",
    "input: (batch-size, channel, data-length)\n",
    "(32, 1, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(2,), stride=(1,))\n",
      "  (maxpool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Some Information about MyModule\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, groups=1, bias=True, kernel_size=20, stride=1)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=16, groups=1, bias=True, kernel_size=5, stride=1)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(2 * 16, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # print('conv1 Shape: {}'.format(x.shape))\n",
    "        x = self.maxpool1(x)\n",
    "        # print('MaxPool1 Shape: {}'.format(x.shape))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # print('conv2 Shape: {}'.format(x.shape))\n",
    "        x = self.maxpool2(x)\n",
    "        # print('MaxPool2 Shape: {}'.format(x.shape))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # print('Flaten Shape: {}'.format(x.shape))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print('Linear1 Shape: {}'.format(x.shape))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = ConvNet()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "丟進model需要為`Tensor`型態. \n",
    "\n",
    "inputs, labels: `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 23056.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([68])) that is different to the input size (torch.Size([68, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     1] loss: 749.317\n",
      "[3,     1] loss: 747.980\n",
      "[4,     1] loss: 747.552\n",
      "[5,     1] loss: 748.636\n",
      "[6,     1] loss: 749.139\n",
      "[7,     1] loss: 749.493\n",
      "[8,     1] loss: 749.746\n",
      "[9,     1] loss: 749.931\n",
      "[10,     1] loss: 750.066\n",
      "[11,     1] loss: 750.169\n",
      "[12,     1] loss: 750.248\n",
      "[13,     1] loss: 750.309\n",
      "[14,     1] loss: 750.358\n",
      "[15,     1] loss: 750.396\n",
      "[16,     1] loss: 750.428\n",
      "[17,     1] loss: 750.453\n",
      "[18,     1] loss: 750.473\n",
      "[19,     1] loss: 750.493\n",
      "[20,     1] loss: 750.505\n",
      "[21,     1] loss: 750.516\n",
      "[22,     1] loss: 750.525\n",
      "[23,     1] loss: 750.534\n",
      "[24,     1] loss: 750.541\n",
      "[25,     1] loss: 750.547\n",
      "[26,     1] loss: 750.553\n",
      "[27,     1] loss: 750.559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_463/2320937412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_463/986045688.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# print('MaxPool1 Shape: {}'.format(x.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    303\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 304\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "hist = np.zeros(epochs)\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs = torch.from_numpy(x)\n",
    "        # labels = torch.from_numpy(y)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # calculate dl/dw\n",
    "        optimizer.step() # update weights\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # if (i % (len(train_loader) // 4) == 0) or (i == len(train_loader)-1):    # print loss every 5 mini-batches\n",
    "        #   print(f'[ epoch: {epoch + 1}, batch: {i + 1:5d}] loss: {running_loss / 5:.3f}')\n",
    "        #   running_loss = 0.0\n",
    "    \n",
    "    print(f'[ epoch: {epoch + 1} ].   MSE: {running_loss}')\n",
    "    hist[epoch] = running_loss\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation\n",
    "\n",
    "計算validation data的RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "pred_value = np.zeros(len(val_loader))\n",
    "true_value = np.zeros(len(val_loader))\n",
    "loss = 0\n",
    "for i, (inputs, labels) in enumerate(val_loader):\n",
    "    pred = net(inputs)\n",
    "    pred = scaler.inverse_transform(pred.detach().numpy())\n",
    "    ans = scaler.inverse_transform(labels.reshape(-1, 1))\n",
    "    pred_value[i] = pred\n",
    "    true_value[i] = ans\n",
    "    loss = (pred - ans) ** 2\n",
    "\n",
    "print(loss/len(val_loader))\n",
    "\n",
    "\n",
    "# Visualising the results\n",
    "figure, axes = plt.subplots(figsize=(15, 6))\n",
    "axes.xaxis_date()\n",
    "\n",
    "axes.plot(df[len(df)-len(val_loader):].index, pred_value, color = 'red', label = 'Predicted Price')\n",
    "axes.plot(df[len(df)-len(val_loader):].index, true_value, color = 'blue', label = 'Real Price')\n",
    "#axes.xticks(np.arange(0,394,50))\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "# plt.savefig('ibm_pred.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
