{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[290., 290., 290., 290., 290.]],\n",
      "\n",
      "        [[290., 290., 290., 290., 270.]],\n",
      "\n",
      "        [[290., 290., 290., 270., 270.]],\n",
      "\n",
      "        [[290., 290., 270., 270., 270.]],\n",
      "\n",
      "        [[290., 270., 270., 270., 270.]],\n",
      "\n",
      "        [[270., 270., 270., 270., 270.]],\n",
      "\n",
      "        [[270., 270., 270., 270., 260.]],\n",
      "\n",
      "        [[270., 270., 270., 260., 260.]],\n",
      "\n",
      "        [[270., 270., 260., 260., 260.]],\n",
      "\n",
      "        [[270., 260., 260., 260., 260.]],\n",
      "\n",
      "        [[260., 260., 260., 260., 265.]],\n",
      "\n",
      "        [[260., 260., 260., 265., 265.]],\n",
      "\n",
      "        [[260., 260., 265., 265., 265.]],\n",
      "\n",
      "        [[260., 265., 265., 265., 260.]],\n",
      "\n",
      "        [[265., 265., 265., 260., 260.]],\n",
      "\n",
      "        [[265., 265., 260., 260., 255.]],\n",
      "\n",
      "        [[265., 260., 260., 255., 245.]],\n",
      "\n",
      "        [[260., 260., 255., 245., 245.]],\n",
      "\n",
      "        [[260., 255., 245., 245., 255.]],\n",
      "\n",
      "        [[255., 245., 245., 255., 255.]],\n",
      "\n",
      "        [[245., 245., 255., 255., 260.]],\n",
      "\n",
      "        [[245., 255., 255., 260., 260.]],\n",
      "\n",
      "        [[255., 255., 260., 260., 275.]],\n",
      "\n",
      "        [[255., 260., 260., 275., 275.]],\n",
      "\n",
      "        [[260., 260., 275., 275., 275.]],\n",
      "\n",
      "        [[260., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 285.]],\n",
      "\n",
      "        [[275., 275., 275., 285., 285.]],\n",
      "\n",
      "        [[275., 275., 285., 285., 285.]],\n",
      "\n",
      "        [[275., 285., 285., 285., 285.]],\n",
      "\n",
      "        [[285., 285., 285., 285., 285.]],\n",
      "\n",
      "        [[285., 285., 285., 285., 285.]],\n",
      "\n",
      "        [[285., 285., 285., 285., 305.]],\n",
      "\n",
      "        [[285., 285., 285., 305., 305.]],\n",
      "\n",
      "        [[285., 285., 305., 305., 310.]],\n",
      "\n",
      "        [[285., 305., 305., 310., 310.]],\n",
      "\n",
      "        [[305., 305., 310., 310., 310.]],\n",
      "\n",
      "        [[305., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 315.]],\n",
      "\n",
      "        [[310., 310., 310., 315., 315.]],\n",
      "\n",
      "        [[310., 310., 315., 315., 315.]],\n",
      "\n",
      "        [[310., 315., 315., 315., 315.]],\n",
      "\n",
      "        [[315., 315., 315., 315., 320.]],\n",
      "\n",
      "        [[315., 315., 315., 320., 320.]],\n",
      "\n",
      "        [[315., 315., 320., 320., 320.]],\n",
      "\n",
      "        [[315., 320., 320., 320., 310.]],\n",
      "\n",
      "        [[320., 320., 320., 310., 310.]],\n",
      "\n",
      "        [[320., 320., 310., 310., 310.]],\n",
      "\n",
      "        [[320., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 315.]],\n",
      "\n",
      "        [[310., 310., 310., 315., 315.]],\n",
      "\n",
      "        [[310., 310., 315., 315., 320.]],\n",
      "\n",
      "        [[310., 315., 315., 320., 320.]],\n",
      "\n",
      "        [[315., 315., 320., 320., 320.]],\n",
      "\n",
      "        [[315., 320., 320., 320., 320.]],\n",
      "\n",
      "        [[320., 320., 320., 320., 320.]],\n",
      "\n",
      "        [[320., 320., 320., 320., 320.]],\n",
      "\n",
      "        [[320., 320., 320., 320., 310.]],\n",
      "\n",
      "        [[320., 320., 320., 310., 310.]],\n",
      "\n",
      "        [[320., 320., 310., 310., 310.]],\n",
      "\n",
      "        [[320., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 305.]],\n",
      "\n",
      "        [[310., 310., 310., 305., 305.]],\n",
      "\n",
      "        [[310., 310., 305., 305., 305.]],\n",
      "\n",
      "        [[310., 305., 305., 305., 305.]],\n",
      "\n",
      "        [[305., 305., 305., 305., 305.]],\n",
      "\n",
      "        [[305., 305., 305., 305., 310.]],\n",
      "\n",
      "        [[305., 305., 305., 310., 310.]],\n",
      "\n",
      "        [[305., 305., 310., 310., 310.]],\n",
      "\n",
      "        [[305., 310., 310., 310., 315.]],\n",
      "\n",
      "        [[310., 310., 310., 315., 320.]],\n",
      "\n",
      "        [[310., 310., 315., 320., 325.]],\n",
      "\n",
      "        [[310., 315., 320., 325., 335.]],\n",
      "\n",
      "        [[315., 320., 325., 335., 330.]],\n",
      "\n",
      "        [[320., 325., 335., 330., 325.]],\n",
      "\n",
      "        [[325., 335., 330., 325., 325.]],\n",
      "\n",
      "        [[335., 330., 325., 325., 325.]],\n",
      "\n",
      "        [[330., 325., 325., 325., 325.]],\n",
      "\n",
      "        [[325., 325., 325., 325., 325.]],\n",
      "\n",
      "        [[325., 325., 325., 325., 340.]],\n",
      "\n",
      "        [[325., 325., 325., 340., 340.]],\n",
      "\n",
      "        [[325., 325., 340., 340., 340.]],\n",
      "\n",
      "        [[325., 340., 340., 340., 340.]],\n",
      "\n",
      "        [[340., 340., 340., 340., 340.]],\n",
      "\n",
      "        [[340., 340., 340., 340., 340.]],\n",
      "\n",
      "        [[340., 340., 340., 340., 340.]],\n",
      "\n",
      "        [[340., 340., 340., 340., 345.]],\n",
      "\n",
      "        [[340., 340., 340., 345., 345.]],\n",
      "\n",
      "        [[340., 340., 345., 345., 345.]],\n",
      "\n",
      "        [[340., 345., 345., 345., 345.]],\n",
      "\n",
      "        [[345., 345., 345., 345., 360.]],\n",
      "\n",
      "        [[345., 345., 345., 360., 365.]],\n",
      "\n",
      "        [[345., 345., 360., 365., 375.]],\n",
      "\n",
      "        [[345., 360., 365., 375., 375.]],\n",
      "\n",
      "        [[360., 365., 375., 375., 375.]],\n",
      "\n",
      "        [[365., 375., 375., 375., 375.]],\n",
      "\n",
      "        [[375., 375., 375., 375., 370.]],\n",
      "\n",
      "        [[375., 375., 375., 370., 370.]],\n",
      "\n",
      "        [[375., 375., 370., 370., 370.]],\n",
      "\n",
      "        [[375., 370., 370., 370., 385.]],\n",
      "\n",
      "        [[370., 370., 370., 385., 390.]],\n",
      "\n",
      "        [[370., 370., 385., 390., 390.]],\n",
      "\n",
      "        [[370., 385., 390., 390., 385.]],\n",
      "\n",
      "        [[385., 390., 390., 385., 385.]],\n",
      "\n",
      "        [[390., 390., 385., 385., 380.]],\n",
      "\n",
      "        [[390., 385., 385., 380., 380.]],\n",
      "\n",
      "        [[385., 385., 380., 380., 380.]],\n",
      "\n",
      "        [[385., 380., 380., 380., 380.]],\n",
      "\n",
      "        [[380., 380., 380., 380., 380.]],\n",
      "\n",
      "        [[380., 380., 380., 380., 380.]],\n",
      "\n",
      "        [[380., 380., 380., 380., 390.]],\n",
      "\n",
      "        [[380., 380., 380., 390., 390.]],\n",
      "\n",
      "        [[380., 380., 390., 390., 390.]],\n",
      "\n",
      "        [[380., 390., 390., 390., 400.]],\n",
      "\n",
      "        [[390., 390., 390., 400., 400.]],\n",
      "\n",
      "        [[390., 390., 400., 400., 400.]],\n",
      "\n",
      "        [[390., 400., 400., 400., 400.]],\n",
      "\n",
      "        [[400., 400., 400., 400., 400.]],\n",
      "\n",
      "        [[400., 400., 400., 400., 400.]],\n",
      "\n",
      "        [[400., 400., 400., 400., 400.]],\n",
      "\n",
      "        [[400., 400., 400., 400., 400.]]]) tensor([270., 270., 270., 270., 270., 260., 260., 260., 260., 265., 265., 265.,\n",
      "        260., 260., 255., 245., 245., 255., 255., 260., 260., 275., 275., 275.,\n",
      "        275., 275., 275., 275., 275., 285., 285., 285., 285., 285., 285., 305.,\n",
      "        305., 310., 310., 310., 310., 315., 315., 315., 315., 320., 320., 320.,\n",
      "        310., 310., 310., 310., 310., 310., 315., 315., 320., 320., 320., 320.,\n",
      "        320., 320., 310., 310., 310., 310., 310., 305., 305., 305., 305., 305.,\n",
      "        310., 310., 310., 315., 320., 325., 335., 330., 325., 325., 325., 325.,\n",
      "        325., 340., 340., 340., 340., 340., 340., 340., 345., 345., 345., 345.,\n",
      "        360., 365., 375., 375., 375., 375., 370., 370., 370., 385., 390., 390.,\n",
      "        385., 385., 380., 380., 380., 380., 380., 380., 390., 390., 390., 400.,\n",
      "        400., 400., 400., 400., 400., 400., 400., 400.])\n",
      "torch.Size([128, 1, 5]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "class ClosePriceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about ClosePriceDataset\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ClosePriceDataset, self).__init__()\n",
    "        data = pd.read_csv('../datasets/BRK-A.csv')\n",
    "        n_steps = 5\n",
    "        self.x, self.y = self.split_sequence(data['Close'], n_steps)\n",
    "        self.n_sample = self.x.size(dim=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_sample\n",
    "\n",
    "    def split_sequence(slef, sequence: pd.Series, timestep: int):\n",
    "        X = list()\n",
    "        y = list()\n",
    "\n",
    "        for i in range(len(sequence)):\n",
    "            end_idx = i + timestep\n",
    "            if end_idx > len(sequence) - 1:\n",
    "                break\n",
    "            seq_X, seq_y = [sequence[i:end_idx]], sequence[end_idx]\n",
    "            X.append(seq_X)\n",
    "            y.append(seq_y)\n",
    "\n",
    "\n",
    "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "def data_split(dataset, val_split = 0.25, random_seed = 0):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    val_idx = int(np.floor(dataset_size * val_split))\n",
    "    train_indices, val_indices = indices[val_idx:], indices[:val_idx]\n",
    "    train_sampler = SequentialSampler(train_indices)\n",
    "    valid_sampler = SequentialSampler(val_indices)\n",
    "\n",
    "    return train_sampler, valid_sampler\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "val_split = 0.25\n",
    "random_seed= 42\n",
    "\n",
    "dataset = ClosePriceDataset()\n",
    "train_sampler, valid_sampler = data_split(dataset=dataset, val_split=val_split, random_seed=random_seed)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features, labels = data\n",
    "print(features, labels)\n",
    "print(features.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(2,), stride=(1,))\n",
      "  (maxpool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Some Information about MyModule\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, groups=1, bias=True, kernel_size=2, stride=1)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(2 * 64, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(F.relu(self.conv1(x)))\n",
    "        # print('MaxPool1 Shape: {}'.format(x.shape))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # print('Flaten Shape: {}'.format(x.shape))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print('Linear1 Shape: {}'.format(x.shape))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = ConvNet()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 23056.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([68])) that is different to the input size (torch.Size([68, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     1] loss: 749.317\n",
      "[3,     1] loss: 747.980\n",
      "[4,     1] loss: 747.552\n",
      "[5,     1] loss: 748.636\n",
      "[6,     1] loss: 749.139\n",
      "[7,     1] loss: 749.493\n",
      "[8,     1] loss: 749.746\n",
      "[9,     1] loss: 749.931\n",
      "[10,     1] loss: 750.066\n",
      "[11,     1] loss: 750.169\n",
      "[12,     1] loss: 750.248\n",
      "[13,     1] loss: 750.309\n",
      "[14,     1] loss: 750.358\n",
      "[15,     1] loss: 750.396\n",
      "[16,     1] loss: 750.428\n",
      "[17,     1] loss: 750.453\n",
      "[18,     1] loss: 750.473\n",
      "[19,     1] loss: 750.493\n",
      "[20,     1] loss: 750.505\n",
      "[21,     1] loss: 750.516\n",
      "[22,     1] loss: 750.525\n",
      "[23,     1] loss: 750.534\n",
      "[24,     1] loss: 750.541\n",
      "[25,     1] loss: 750.547\n",
      "[26,     1] loss: 750.553\n",
      "[27,     1] loss: 750.559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_463/2320937412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_463/986045688.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# print('MaxPool1 Shape: {}'.format(x.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    303\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 304\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs = torch.from_numpy(x)\n",
    "        # labels = torch.from_numpy(y)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 64 == 0:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 5:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
