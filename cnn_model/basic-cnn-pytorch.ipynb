{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Date\n",
    "\n",
    "- `class ClosePriceDataset()`: `Dataset`\n",
    "    - `__init__`: read_csv()\n",
    "        - `MinMaxScaler`: [-1, 1]，(可以被reverse回原值)\n",
    "- `def train_valid_split`: return `Sampler`, split training data(80%) and validation data(20%)\n",
    "- `train_loader`: return `DataLoader`\n",
    "- `val_loader`: return `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[290., 290., 290., 290., 290.]],\n",
      "\n",
      "        [[290., 290., 290., 290., 270.]],\n",
      "\n",
      "        [[290., 290., 290., 270., 270.]],\n",
      "\n",
      "        [[290., 290., 270., 270., 270.]],\n",
      "\n",
      "        [[290., 270., 270., 270., 270.]],\n",
      "\n",
      "        [[270., 270., 270., 270., 270.]],\n",
      "\n",
      "        [[270., 270., 270., 270., 260.]],\n",
      "\n",
      "        [[270., 270., 270., 260., 260.]],\n",
      "\n",
      "        [[270., 270., 260., 260., 260.]],\n",
      "\n",
      "        [[270., 260., 260., 260., 260.]],\n",
      "\n",
      "        [[260., 260., 260., 260., 265.]],\n",
      "\n",
      "        [[260., 260., 260., 265., 265.]],\n",
      "\n",
      "        [[260., 260., 265., 265., 265.]],\n",
      "\n",
      "        [[260., 265., 265., 265., 260.]],\n",
      "\n",
      "        [[265., 265., 265., 260., 260.]],\n",
      "\n",
      "        [[265., 265., 260., 260., 255.]],\n",
      "\n",
      "        [[265., 260., 260., 255., 245.]],\n",
      "\n",
      "        [[260., 260., 255., 245., 245.]],\n",
      "\n",
      "        [[260., 255., 245., 245., 255.]],\n",
      "\n",
      "        [[255., 245., 245., 255., 255.]],\n",
      "\n",
      "        [[245., 245., 255., 255., 260.]],\n",
      "\n",
      "        [[245., 255., 255., 260., 260.]],\n",
      "\n",
      "        [[255., 255., 260., 260., 275.]],\n",
      "\n",
      "        [[255., 260., 260., 275., 275.]],\n",
      "\n",
      "        [[260., 260., 275., 275., 275.]],\n",
      "\n",
      "        [[260., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 275.]],\n",
      "\n",
      "        [[275., 275., 275., 275., 285.]],\n",
      "\n",
      "        [[275., 275., 275., 285., 285.]],\n",
      "\n",
      "        [[275., 275., 285., 285., 285.]],\n",
      "\n",
      "        [[275., 285., 285., 285., 285.]],\n",
      "\n",
      "        [[285., 285., 285., 285., 285.]],\n",
      "\n",
      "        [[285., 285., 285., 285., 285.]],\n",
      "\n",
      "        [[285., 285., 285., 285., 305.]],\n",
      "\n",
      "        [[285., 285., 285., 305., 305.]],\n",
      "\n",
      "        [[285., 285., 305., 305., 310.]],\n",
      "\n",
      "        [[285., 305., 305., 310., 310.]],\n",
      "\n",
      "        [[305., 305., 310., 310., 310.]],\n",
      "\n",
      "        [[305., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 315.]],\n",
      "\n",
      "        [[310., 310., 310., 315., 315.]],\n",
      "\n",
      "        [[310., 310., 315., 315., 315.]],\n",
      "\n",
      "        [[310., 315., 315., 315., 315.]],\n",
      "\n",
      "        [[315., 315., 315., 315., 320.]],\n",
      "\n",
      "        [[315., 315., 315., 320., 320.]],\n",
      "\n",
      "        [[315., 315., 320., 320., 320.]],\n",
      "\n",
      "        [[315., 320., 320., 320., 310.]],\n",
      "\n",
      "        [[320., 320., 320., 310., 310.]],\n",
      "\n",
      "        [[320., 320., 310., 310., 310.]],\n",
      "\n",
      "        [[320., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 315.]],\n",
      "\n",
      "        [[310., 310., 310., 315., 315.]],\n",
      "\n",
      "        [[310., 310., 315., 315., 320.]],\n",
      "\n",
      "        [[310., 315., 315., 320., 320.]],\n",
      "\n",
      "        [[315., 315., 320., 320., 320.]],\n",
      "\n",
      "        [[315., 320., 320., 320., 320.]],\n",
      "\n",
      "        [[320., 320., 320., 320., 320.]],\n",
      "\n",
      "        [[320., 320., 320., 320., 320.]],\n",
      "\n",
      "        [[320., 320., 320., 320., 310.]],\n",
      "\n",
      "        [[320., 320., 320., 310., 310.]],\n",
      "\n",
      "        [[320., 320., 310., 310., 310.]],\n",
      "\n",
      "        [[320., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 310.]],\n",
      "\n",
      "        [[310., 310., 310., 310., 305.]],\n",
      "\n",
      "        [[310., 310., 310., 305., 305.]],\n",
      "\n",
      "        [[310., 310., 305., 305., 305.]],\n",
      "\n",
      "        [[310., 305., 305., 305., 305.]],\n",
      "\n",
      "        [[305., 305., 305., 305., 305.]],\n",
      "\n",
      "        [[305., 305., 305., 305., 310.]],\n",
      "\n",
      "        [[305., 305., 305., 310., 310.]],\n",
      "\n",
      "        [[305., 305., 310., 310., 310.]],\n",
      "\n",
      "        [[305., 310., 310., 310., 315.]],\n",
      "\n",
      "        [[310., 310., 310., 315., 320.]],\n",
      "\n",
      "        [[310., 310., 315., 320., 325.]],\n",
      "\n",
      "        [[310., 315., 320., 325., 335.]],\n",
      "\n",
      "        [[315., 320., 325., 335., 330.]],\n",
      "\n",
      "        [[320., 325., 335., 330., 325.]],\n",
      "\n",
      "        [[325., 335., 330., 325., 325.]],\n",
      "\n",
      "        [[335., 330., 325., 325., 325.]],\n",
      "\n",
      "        [[330., 325., 325., 325., 325.]],\n",
      "\n",
      "        [[325., 325., 325., 325., 325.]],\n",
      "\n",
      "        [[325., 325., 325., 325., 340.]],\n",
      "\n",
      "        [[325., 325., 325., 340., 340.]],\n",
      "\n",
      "        [[325., 325., 340., 340., 340.]],\n",
      "\n",
      "        [[325., 340., 340., 340., 340.]],\n",
      "\n",
      "        [[340., 340., 340., 340., 340.]],\n",
      "\n",
      "        [[340., 340., 340., 340., 340.]],\n",
      "\n",
      "        [[340., 340., 340., 340., 340.]],\n",
      "\n",
      "        [[340., 340., 340., 340., 345.]],\n",
      "\n",
      "        [[340., 340., 340., 345., 345.]],\n",
      "\n",
      "        [[340., 340., 345., 345., 345.]],\n",
      "\n",
      "        [[340., 345., 345., 345., 345.]],\n",
      "\n",
      "        [[345., 345., 345., 345., 360.]],\n",
      "\n",
      "        [[345., 345., 345., 360., 365.]],\n",
      "\n",
      "        [[345., 345., 360., 365., 375.]],\n",
      "\n",
      "        [[345., 360., 365., 375., 375.]],\n",
      "\n",
      "        [[360., 365., 375., 375., 375.]],\n",
      "\n",
      "        [[365., 375., 375., 375., 375.]],\n",
      "\n",
      "        [[375., 375., 375., 375., 370.]],\n",
      "\n",
      "        [[375., 375., 375., 370., 370.]],\n",
      "\n",
      "        [[375., 375., 370., 370., 370.]],\n",
      "\n",
      "        [[375., 370., 370., 370., 385.]],\n",
      "\n",
      "        [[370., 370., 370., 385., 390.]],\n",
      "\n",
      "        [[370., 370., 385., 390., 390.]],\n",
      "\n",
      "        [[370., 385., 390., 390., 385.]],\n",
      "\n",
      "        [[385., 390., 390., 385., 385.]],\n",
      "\n",
      "        [[390., 390., 385., 385., 380.]],\n",
      "\n",
      "        [[390., 385., 385., 380., 380.]],\n",
      "\n",
      "        [[385., 385., 380., 380., 380.]],\n",
      "\n",
      "        [[385., 380., 380., 380., 380.]],\n",
      "\n",
      "        [[380., 380., 380., 380., 380.]],\n",
      "\n",
      "        [[380., 380., 380., 380., 380.]],\n",
      "\n",
      "        [[380., 380., 380., 380., 390.]],\n",
      "\n",
      "        [[380., 380., 380., 390., 390.]],\n",
      "\n",
      "        [[380., 380., 390., 390., 390.]],\n",
      "\n",
      "        [[380., 390., 390., 390., 400.]],\n",
      "\n",
      "        [[390., 390., 390., 400., 400.]],\n",
      "\n",
      "        [[390., 390., 400., 400., 400.]],\n",
      "\n",
      "        [[390., 400., 400., 400., 400.]],\n",
      "\n",
      "        [[400., 400., 400., 400., 400.]],\n",
      "\n",
      "        [[400., 400., 400., 400., 400.]],\n",
      "\n",
      "        [[400., 400., 400., 400., 400.]],\n",
      "\n",
      "        [[400., 400., 400., 400., 400.]]]) tensor([270., 270., 270., 270., 270., 260., 260., 260., 260., 265., 265., 265.,\n",
      "        260., 260., 255., 245., 245., 255., 255., 260., 260., 275., 275., 275.,\n",
      "        275., 275., 275., 275., 275., 285., 285., 285., 285., 285., 285., 305.,\n",
      "        305., 310., 310., 310., 310., 315., 315., 315., 315., 320., 320., 320.,\n",
      "        310., 310., 310., 310., 310., 310., 315., 315., 320., 320., 320., 320.,\n",
      "        320., 320., 310., 310., 310., 310., 310., 305., 305., 305., 305., 305.,\n",
      "        310., 310., 310., 315., 320., 325., 335., 330., 325., 325., 325., 325.,\n",
      "        325., 340., 340., 340., 340., 340., 340., 340., 345., 345., 345., 345.,\n",
      "        360., 365., 375., 375., 375., 375., 370., 370., 370., 385., 390., 390.,\n",
      "        385., 385., 380., 380., 380., 380., 380., 380., 390., 390., 390., 400.,\n",
      "        400., 400., 400., 400., 400., 400., 400., 400.])\n",
      "torch.Size([128, 1, 5]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "class ClosePriceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    self.n_steps: int\n",
    "    self.n_days, self.label: ndarray\n",
    "    \"\"\"\n",
    "    def __init__(self, root, n_steps):\n",
    "        super(ClosePriceDataset, self).__init__()\n",
    "        data = pd.read_csv(root, index_col='Date')\n",
    "        data = data[['Close']]\n",
    "        # data.info()\n",
    "\n",
    "        self.n_steps = n_steps\n",
    "        # self.n_days = scaler.fit_transform(self.n_days.values.reshape(-1, 1))\n",
    "        scaled_prices = scaler.fit_transform(data.values)\n",
    "        scaled_prices = scaled_prices.reshape((-1,))\n",
    "        # print(f'total days: {scaled_prices.shape}')\n",
    "        self.n_days, self.label = self.split_sequence(scaled_prices, n_steps)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(f'n_days: {self.n_days[index]}, label: {self.label[index]}')\n",
    "        return self.n_days[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def split_sequence(slef, scaled_sequence, timestep):\n",
    "        \"\"\"\n",
    "        return X: ndarray (*, 1, timestep)\n",
    "        return y: ndarray (1,)\n",
    "        \"\"\"\n",
    "        X = list()\n",
    "        y = list()\n",
    "\n",
    "        for i in range(len(scaled_sequence)):\n",
    "            end_idx = i + timestep\n",
    "            if end_idx > len(scaled_sequence) - 1:\n",
    "                break\n",
    "            seq_X, seq_y = [scaled_sequence[i:end_idx]], scaled_sequence[end_idx]\n",
    "            X.append(seq_X)\n",
    "            y.append(seq_y)\n",
    "\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # print out first record\n",
    "        # print(X[0:2])\n",
    "        # print(y[0:2])\n",
    "        return torch.Tensor(X), torch.Tensor(y)\n",
    "\n",
    "\n",
    "def train_valid_split(dataset, val_split = 0.2, random_seed = 0):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    val_idx = int(np.floor(dataset_size * val_split))\n",
    "    train_indices, val_indices = indices[val_idx:], indices[:val_idx]\n",
    "    train_sampler = SequentialSampler(train_indices)\n",
    "    valid_sampler = SequentialSampler(val_indices)\n",
    "\n",
    "    return train_sampler, valid_sampler\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "val_split = 0.2\n",
    "random_seed= 42\n",
    "\n",
    "dataset = ClosePriceDataset(root = 'datasets/BRK-A.csv', n_steps = 60)\n",
    "train_sampler, valid_sampler = train_valid_split(dataset=dataset, val_split=val_split, random_seed=random_seed)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=1, sampler=valid_sampler)\n",
    "\n",
    "print(\"train size: \", len(train_sampler))\n",
    "print(\"valid size: \", len(valid_sampler))\n",
    "print(f'(training data) batch size: {batch_size}, #batches: {len(train_loader)}')\n",
    "\n",
    "# dataiter = iter(train_loader)\n",
    "# data = dataiter.next()\n",
    "# features, labels = data\n",
    "# print(scaler.inverse_transform(features.reshape(-1, 1)), scaler.inverse_transform(labels.reshape(1, -1)))\n",
    "# print(features.shape, labels.shape)\n",
    "\n",
    "# dataiter = iter(val_loader)\n",
    "# data = dataiter.next()\n",
    "# features, labels = data\n",
    "# print(features.shape, labels.shape)\n",
    "# print(scaler.inverse_transform(labels.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "每一層需給定的input（計算上一層output）的size\n",
    "\n",
    "input: (batch-size, channel, data-length)\n",
    "(32, 1, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(2,), stride=(1,))\n",
      "  (maxpool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Some Information about MyModule\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, groups=1, bias=True, kernel_size=20, stride=1)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=16, groups=1, bias=True, kernel_size=5, stride=1)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(2 * 16, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # print('conv1 Shape: {}'.format(x.shape))\n",
    "        x = self.maxpool1(x)\n",
    "        # print('MaxPool1 Shape: {}'.format(x.shape))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # print('conv2 Shape: {}'.format(x.shape))\n",
    "        x = self.maxpool2(x)\n",
    "        # print('MaxPool2 Shape: {}'.format(x.shape))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # print('Flaten Shape: {}'.format(x.shape))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print('Linear1 Shape: {}'.format(x.shape))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = ConvNet()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "丟進model需要為`Tensor`型態. \n",
    "\n",
    "inputs, labels: `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 23056.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([68])) that is different to the input size (torch.Size([68, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     1] loss: 749.317\n",
      "[3,     1] loss: 747.980\n",
      "[4,     1] loss: 747.552\n",
      "[5,     1] loss: 748.636\n",
      "[6,     1] loss: 749.139\n",
      "[7,     1] loss: 749.493\n",
      "[8,     1] loss: 749.746\n",
      "[9,     1] loss: 749.931\n",
      "[10,     1] loss: 750.066\n",
      "[11,     1] loss: 750.169\n",
      "[12,     1] loss: 750.248\n",
      "[13,     1] loss: 750.309\n",
      "[14,     1] loss: 750.358\n",
      "[15,     1] loss: 750.396\n",
      "[16,     1] loss: 750.428\n",
      "[17,     1] loss: 750.453\n",
      "[18,     1] loss: 750.473\n",
      "[19,     1] loss: 750.493\n",
      "[20,     1] loss: 750.505\n",
      "[21,     1] loss: 750.516\n",
      "[22,     1] loss: 750.525\n",
      "[23,     1] loss: 750.534\n",
      "[24,     1] loss: 750.541\n",
      "[25,     1] loss: 750.547\n",
      "[26,     1] loss: 750.553\n",
      "[27,     1] loss: 750.559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_463/2320937412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_463/986045688.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# print('MaxPool1 Shape: {}'.format(x.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    303\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 304\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "hist = np.zeros(epochs)\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs = torch.from_numpy(x)\n",
    "        # labels = torch.from_numpy(y)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # calculate dl/dw\n",
    "        optimizer.step() # update weights\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # if (i % (len(train_loader) // 4) == 0) or (i == len(train_loader)-1):    # print loss every 5 mini-batches\n",
    "        #   print(f'[ epoch: {epoch + 1}, batch: {i + 1:5d}] loss: {running_loss / 5:.3f}')\n",
    "        #   running_loss = 0.0\n",
    "    \n",
    "    print(f'[ epoch: {epoch + 1} ].   MSE: {running_loss}')\n",
    "    hist[epoch] = running_loss\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation\n",
    "\n",
    "計算validation data的RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "pred_value = np.zeros(len(val_loader))\n",
    "true_value = np.zeros(len(val_loader))\n",
    "loss = 0\n",
    "for i, (inputs, labels) in enumerate(val_loader):\n",
    "    pred = net(inputs)\n",
    "    pred = scaler.inverse_transform(pred.detach().numpy())\n",
    "    ans = scaler.inverse_transform(labels.reshape(-1, 1))\n",
    "    pred_value[i] = pred\n",
    "    true_value[i] = ans\n",
    "    loss = (pred - ans) ** 2\n",
    "\n",
    "print(loss/len(val_loader))\n",
    "\n",
    "\n",
    "# Visualising the results\n",
    "figure, axes = plt.subplots(figsize=(15, 6))\n",
    "axes.xaxis_date()\n",
    "\n",
    "axes.plot(df[len(df)-len(val_loader):].index, pred_value, color = 'red', label = 'Predicted Price')\n",
    "axes.plot(df[len(df)-len(val_loader):].index, true_value, color = 'blue', label = 'Real Price')\n",
    "#axes.xticks(np.arange(0,394,50))\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "# plt.savefig('ibm_pred.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
